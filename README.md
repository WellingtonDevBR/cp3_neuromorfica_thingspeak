# Introduction

This tutorial plans to go through how to get sensor data from <https://www.thingspeak.com>. The sensor data is generated by sensor IoT devices spread worldwide, and the data is sent to the mentioned website.

We plan to build an application that collects data from many IoT devices, and this data is hosted in a channel over the website. We collect the data from the channel and manage it in a python application. The application manages all the data and connects to a database and sends the data collected to the database. 

There will be a job running every 30 minutes, to check if the sensors sent newer data so it can be stored again.

The technologies used for this application are:
* PYTHON
* Kubernetes
* MYSQL

## Installation

For simplicity, we go for [k3d](https://k3d.io/v5.4.6/). You can install it on windows, macOS, and Linux.

```bash
Linux:
wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash
OR
curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash

Windows
choco install k3d

MacOs
brew install k3d
```
As you have noticed, you must install [chocolatey](https://docs.chocolatey.org/en-us/choco/setup) package manager.

Once we install everything. We create the first cluster that consists of 1 control plane and 4 workers.

```bash
k3d cluster create a cluster --agents 4
```

These settings allow us to start our cluster with 4 workers, that will handle our application and certify its stability of it in case any machine dies.

Now. we set up all or files that will host our application inside our cluster by entering
```basg
kubectl apply -f ./k8s/
```

This applies to all configurations of our application and service for Kubernetes. 
The command must be executed inside the root file of the application.

## Usage

```python
import time
from logger import get_module_logger
import schedule
from decouple import config
from repository.mysql_database import MYSQL
from sensors.air.air import save_air_sensor_data
from sensors.air_polution.air_polution import save_air_polution_sensor_data
from sensors.air_quality.air_quality import save_air_quality_sensor_data
from sensors.co2_measurement.co2_mesurement import save_co2_measurement_sensor_data
from sensors.shake_traffic.shake_traffic import save_shake_traffic_sensor_data
from sensors.tempture.tempture import save_temperature_sensor_data
from sensors.weather.weather import save_weather_sensor_data
from sensors.wind_power.wind_power import save_wind_sensor_data


def job():
    try:
        get_module_logger('main').info('Start job')
        save_air_sensor_data()
        save_air_polution_sensor_data()
        save_air_quality_sensor_data()
        save_co2_measurement_sensor_data()
        save_shake_traffic_sensor_data()
        save_temperature_sensor_data()
        save_weather_sensor_data()
        save_wind_sensor_data()
        get_module_logger('main').info('End job')
    except Exception as e:
        get_module_logger('main').error(e)


# you can use seconds too for testing purposes
# you can see the incomming calls in main.py console
schedule.every(30).minutes.do(job)

while(1):
    schedule.run_pending()
    time.sleep(1)
```



This is our main.py file. We can see what will happen once we run the application. We run our job every 30 minutes and based on inside implementation we save the sensor data in the MySQL database.

## Contributing

Pull requests are welcome. For significant changes, please open an issue first
to discuss what you would like to change.

Please make sure to update tests as appropriate.

## License

[MIT](https://choosealicense.com/licenses/mit/)